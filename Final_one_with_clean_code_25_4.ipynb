{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaD-55/EduCarePCOS/blob/main/Final_one_with_clean_code_25_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) **Importing Dataset from Kaggle**:\n",
        "To begin the PCOS detection and assessment\n",
        "system, I first retrieved a reliable dataset from Kaggle, which includes patient information with and without PCOS. This step ensures that the analysis and machine learning models are based on a well-known and publicly accessible dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "YQtsUNI9Zg25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Dataset from Kaggle\n",
        "import kagglehub\n",
        "prasoonkottarathil_polycystic_ovary_syndrome_pcos_path = kagglehub.dataset_download('prasoonkottarathil/polycystic-ovary-syndrome-pcos')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "0GiMfIltZTzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) **Setting Up the Environment and Loading Packages**: To begin data analysis, I imported essential Python libraries such as numpy for numerical computations and pandas for data handling. Additionally, I used os.walk() to list and verify the input files available in the Kaggle environment under the /kaggle/input directory."
      ],
      "metadata": {
        "id": "6XCWW82MZTEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "5z97XpDqYCNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) **Loading Core Libraries:**\n",
        "These Python libraries are critical for data processing, visualization, and machine learning. They helped me handle everything from data cleaning to model evaluation."
      ],
      "metadata": {
        "id": "HmG6N395YCNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the libraries that I use in this project\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "gRZfBcT-YCNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) **Loading Data**:\n",
        "To import the dataset into Google Colab, I used files.upload() from the google.colab module. This allowed me to upload local files (PCOS_infertility.csv and PCOS_data_without_infertility.xlsx) directly into the Colab environment for further analysis."
      ],
      "metadata": {
        "id": "v1rJEODWYCNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading Files to Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "vMCJT6wMZfjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) **Data Preprocessing**:\n",
        "Steps involved-\n",
        "\n",
        "1. Merging the two files that are sorted into two based on patients with infertility and without infertility\n",
        "\n",
        "2. Dropping the repeated features\n",
        "\n",
        "3. Encoding categorical variables (dtype objects)\n",
        "\n",
        "4. Dealing with missing values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VKxuKT-kYCN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PCOS Datasets\n",
        "import pandas as pd\n",
        "\n",
        "PCOS_inf = pd.read_csv(\"PCOS_infertility.csv\")\n",
        "PCOS_woinf = pd.read_excel(\"PCOS_data_without_infertility.xlsx\", sheet_name=\"Full_new\")\n"
      ],
      "metadata": {
        "id": "T0AuGNUUbAgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merging the two files as per patient file no.\n",
        "#The files were sorted into two based on patients with infertility and without infertility\n",
        "data = pd.merge(PCOS_woinf,PCOS_inf, on='Patient File No.', suffixes=('','_y'),how='left')\n",
        "\n",
        "#Dropping the repeated features after merging\n",
        "data =data.drop(['Unnamed: 44', 'Sl. No_y', 'PCOS (Y/N)_y', '  I   beta-HCG(mIU/mL)_y',\n",
        "       'II    beta-HCG(mIU/mL)_y', 'AMH(ng/mL)_y'], axis=1)\n",
        "\n",
        "#Taking a look at the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "eyLBNrYDbJbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Display Dataset Column Names\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "tbi0s8uaLrFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning, Transformation, and Outlier Removal**: Column names were cleaned for consistency. Numerical age was converted into categorical ranges to match the survey format. Relevant features were selected, missing values removed, and outliers filtered using Z-scores to ensure clean and reliable data for analysis."
      ],
      "metadata": {
        "id": "KXjN8vcUfgCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clean column names (if there are spaces or typos)\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Binning 'Age (yrs)' into age ranges\n",
        "bins = [17, 24, 34, 45]\n",
        "labels = ['18-24', '25-34', '35-45']\n",
        "data['Age Range'] = pd.cut(data['Age (yrs)'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Created List of selected columns (I replaced the numerical age values in the training data with age ranges to align with the format used in the collected survey data, where age was recorded in categorical ranges instead of exact numbers.)\n",
        "selected_columns = [\n",
        "    'PCOS (Y/N)', 'Age Range', 'Weight (Kg)', 'Height(Cm)', 'BMI',\n",
        "    'Cycle(R/I)', 'Waist:Hip Ratio', 'hair growth(Y/N)', 'Skin darkening (Y/N)',\n",
        "    'Hair loss(Y/N)', 'Pimples(Y/N)', 'Reg.Exercise(Y/N)'\n",
        "]\n",
        "\n",
        "# Created a new DataFrame with only the selected columns\n",
        "selected_data = data[selected_columns]\n",
        "\n",
        "# Dropped missing values\n",
        "selected_data = selected_data.dropna()\n",
        "\n",
        "# Added this import to fix the NameError\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Outlier removal only for numeric columns\n",
        "numeric_data = selected_data.select_dtypes(include=[np.number])\n",
        "z_scores = np.abs(zscore(numeric_data))\n",
        "threshold = 3\n",
        "filtered_data = selected_data[(z_scores < threshold).all(axis=1)]\n",
        "\n",
        "print(\"Original shape:\", selected_data.shape)\n",
        "print(\"Filtered shape:\", filtered_data.shape)\n",
        "\n",
        "# Visual check\n",
        "filtered_data.head()\n"
      ],
      "metadata": {
        "id": "0anqkLKYe2AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values - Drop rows with any missing values in selected data\n",
        "selected_data = selected_data.dropna()\n"
      ],
      "metadata": {
        "id": "LvXsceJxe8Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset structure overview of selected columns\n",
        "selected_data.info()\n"
      ],
      "metadata": {
        "id": "ryY_ZcgNftFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview of Preprocessed Dataset of selected columns\n",
        "selected_data.head()\n"
      ],
      "metadata": {
        "id": "kKPU5Ojhfy1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6) **Descriptive Statistics Overview**: This step was performed to get a summary of the dataset’s key statistical properties such as mean, median, standard deviation, and range. It helps identify data distribution, spot anomalies, and guide preprocessing decisions like outlier removal.\n",
        "Steps involved-\n",
        "\n",
        "\n",
        "1.   Descriptive Statistics\n",
        "2.   Outlier Detection and Removal\n",
        "3.   Correlation Analysis (Heatmap)\n",
        "4.   Feature Correlation with Target (PCOS Y/N)\n",
        "5.   Statistical Significance Testing\n",
        "6.   Visualization of Key Features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MKQb0ml6YCN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics (mean, std, min, 25%, 50%, 75%, max)\n",
        "\n",
        "selected_data.describe()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qUjQiwEaYCN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) **Outlier Detection and Removal using Z-Score Method**: To improve data quality and ensure accurate model training, I applied the Z-score method to detect and remove outliers from the dataset. This method identifies data points that deviate significantly from the mean (beyond 3 standard deviations), which could otherwise skew the analysis. Only numeric columns were used for this step, and the filtered dataset was validated by comparing its shape before and after outlier removal."
      ],
      "metadata": {
        "id": "l6IP3oPoMwsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatically Detected and Removed Outliers\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "\n",
        "# Keeping only numeric columns from selected data\n",
        "numeric_data = selected_data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculated Z-scores\n",
        "z_scores = np.abs(zscore(numeric_data))\n",
        "\n",
        "# Defined threshold for outliers\n",
        "threshold = 3\n",
        "\n",
        "# Filtered out rows with any Z-score greater than threshold\n",
        "filtered_data = selected_data[(z_scores < threshold).all(axis=1)]\n",
        "\n",
        "# Checked shape before and after\n",
        "print(\"Original shape:\", selected_data.shape)\n",
        "print(\"Filtered shape:\", filtered_data.shape)\n"
      ],
      "metadata": {
        "id": "M6wzktwKhO8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) **Label Encoding and Correlation Heatmap**: To prepare categorical features for correlation analysis, I used Label Encoding to convert them into numerical format. This enabled a comprehensive heatmap visualization of feature correlations, which helps identify relationships between variables that might influence PCOS prediction. The heatmap provides valuable insights for feature selection and understanding data structure."
      ],
      "metadata": {
        "id": "1sg-emNINkXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Correlation Matrix (Heatmap)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy the data so original stays unchanged\n",
        "encoded_data = filtered_data.copy()\n",
        "\n",
        "# Select categorical columns (object or category types)\n",
        "categorical_cols = encoded_data.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Convert all categorical columns to strings first (in case of categories)\n",
        "encoded_data[categorical_cols] = encoded_data[categorical_cols].astype(str)\n",
        "\n",
        "# Apply Label Encoding\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    encoded_data[col] = le.fit_transform(encoded_data[col])\n",
        "\n",
        "#Creating heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(encoded_data.corr(), annot=True, cmap='viridis', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap (Including Encoded Categorical Features)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CmhiZRZ_ibv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) **Feature Correlation with PCOS Outcome**: This step analyzes how each feature correlates with the target variable ‘PCOS (Y/N)’. By sorting the correlation values, I identified which features have stronger positive or negative associations with PCOS, aiding in feature selection and understanding potential predictors."
      ],
      "metadata": {
        "id": "zaKvYaClOYKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation of all features with PCOS (Y/N)\n",
        "pcos_correlations = encoded_data.corr()['PCOS (Y/N)'].sort_values(ascending=False)\n",
        "\n",
        "# Display correlations\n",
        "print(\"Feature Correlation with PCOS (Y/N):\")\n",
        "print(pcos_correlations)\n",
        "\n"
      ],
      "metadata": {
        "id": "DcbKGZXnjdx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 10 correlated features\n",
        "plt.figure(figsize=(10, 6))\n",
        "pcos_correlations.drop('PCOS (Y/N)').head(10).plot(kind='barh', color='teal')\n",
        "plt.title('Top 10 Features Correlated with PCOS (Y/N)')\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.gca().invert_yaxis()  # Highest correlation on top\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tx2Pw3SK2p0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) **Statistical Significance Testing of Features Related to PCOS**: To determine which features have a statistically significant association with PCOS, I applied independent t-tests for numerical variables (e.g., BMI, Waist:Hip Ratio) and Chi-square tests for categorical variables (e.g., Cycle type, Hair loss, Age Range). Features with p-values **< 0.05** were considered statistically significant, helping to identify potential predictors for PCOS."
      ],
      "metadata": {
        "id": "RasVzZ7-O4SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assessing assess which features are statistically significant in relation to PCOS (Y/N),\n",
        "#For numerical → used Independent t-test (e.g., BMI, Waist:Hip Ratio)\n",
        "#For categorical → used Chi-square test (e.g., Cycle(R/I), Hair loss(Y/N), Age Range)\n",
        "\n",
        "from scipy.stats import ttest_ind, chi2_contingency\n",
        "\n",
        "# Separated PCOS and non-PCOS groups\n",
        "pcos_group = encoded_data[encoded_data['PCOS (Y/N)'] == 1]\n",
        "non_pcos_group = encoded_data[encoded_data['PCOS (Y/N)'] == 0]\n",
        "\n",
        "# Stored results\n",
        "significance_results = []\n",
        "\n",
        "for col in encoded_data.columns:\n",
        "    if col == 'PCOS (Y/N)':\n",
        "        continue\n",
        "\n",
        "    if encoded_data[col].dtype in ['int64', 'float64']:  # Numerical → T-test\n",
        "        stat, p_val = ttest_ind(pcos_group[col], non_pcos_group[col], equal_var=False)\n",
        "        test_type = 'T-test'\n",
        "    else:  # Categorical → Chi-square\n",
        "        contingency = pd.crosstab(encoded_data[col], encoded_data['PCOS (Y/N)'])\n",
        "        stat, p_val, _, _ = chi2_contingency(contingency)\n",
        "        test_type = 'Chi-square'\n",
        "\n",
        "    significance_results.append({\n",
        "        'Feature': col,\n",
        "        'Test': test_type,\n",
        "        'P-Value': p_val\n",
        "    })\n",
        "\n",
        "# Created DataFrame of results\n",
        "significance_df = pd.DataFrame(significance_results).sort_values('P-Value')\n",
        "\n",
        "# Display\n",
        "print(\"Statistical Significance of Features (sorted by p-value):\")\n",
        "print(significance_df)\n",
        "\n",
        "#Interpretation:\n",
        "#P-value < 0.05 → Statistically significant feature.\n",
        "\n",
        "significant_features = significance_df[significance_df['P-Value'] < 0.05]\n",
        "print(\"Significant features (p < 0.05):\")\n",
        "print(significant_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "wzw-0opEmN_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) **Visualization of Top Significant Features with Respect to PCOS**: Based on the statistical significance analysis, I selected key features (both categorical and numerical) for visual exploration. Count plots were used for categorical features such as Skin darkening, Hair growth, Cycle irregularity, etc., and boxplots for numerical features like Weight and BMI. This helped in visually understanding the distribution and relationship of these features with PCOS diagnosis.\n",
        "\n"
      ],
      "metadata": {
        "id": "ObNQ7Pl9Phw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Top Features to Visualize (from p-values):\n",
        "#I focused on these:\n",
        "#Categorical: Skin darkening (Y/N), hair growth(Y/N), Cycle(R/I), Pimples(Y/N), Hair loss(Y/N), Age Range\n",
        "#Numerical: Weight (Kg), BMI\n",
        "\n",
        "# 1.Categorical Features vs PCOS\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Categorical features to visualize\n",
        "categorical_features = [\n",
        "    'Skin darkening (Y/N)',\n",
        "    'hair growth(Y/N)',\n",
        "    'Cycle(R/I)',\n",
        "    'Pimples(Y/N)',\n",
        "    'Hair loss(Y/N)',\n",
        "    'Age Range'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "for i, col in enumerate(categorical_features, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.countplot(data=encoded_data, x=col, hue='PCOS (Y/N)', palette='Set2')\n",
        "    plt.title(f'{col} by PCOS (Y/N)')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Number of Cases')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Numerical Features vs PCOS\n",
        "\n",
        "# Numerical features to visualize\n",
        "numerical_features = ['Weight (Kg)', 'BMI']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_features, 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    sns.boxplot(data=encoded_data, x='PCOS (Y/N)', y=col, palette='Set2')\n",
        "    plt.title(f'{col} by PCOS (Y/N)')\n",
        "    plt.xlabel('PCOS (Y/N)')\n",
        "    plt.ylabel(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rm0R_DbYnGWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V_z3FRp4PwdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) **Model Building and Evaluation for PCOS Prediction**: This section outlines the process of developing a predictive model for PCOS using selected features. The 'Age Range' variable was label encoded to be used as a numerical input. A Random Forest Classifier was trained after splitting the dataset into training and testing sets (with stratification to maintain class balance). Model performance was assessed using accuracy, confusion matrix, and classification report to evaluate its effectiveness in correctly identifying PCOS cases."
      ],
      "metadata": {
        "id": "DyjazOtKP7Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model building for predicting PCOS\n",
        "\n",
        "# Label encoding for training data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make a copy of the column BEFORE encoding\n",
        "selected_data['Age Range Original'] = selected_data['Age Range']\n",
        "\n",
        "# Initialize the encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit on the original column (must be strings or categories)\n",
        "selected_data['Age Range Encoded'] = le.fit_transform(selected_data['Age Range Original'].astype(str))\n",
        "\n",
        "# Show the label mapping\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label Encoding Mapping:\")\n",
        "print(label_mapping)\n",
        "\n",
        "# View some results\n",
        "print(selected_data[['Age Range Original', 'Age Range Encoded']].head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepared Features and Target\n",
        "X = filtered_data.copy()\n",
        "\n",
        "# First, encode Age Range column (again) in filtered_data if not done already\n",
        "X['Age Range Original'] = X['Age Range']\n",
        "X['Age Range Encoded'] = le.transform(X['Age Range Original'].astype(str))\n",
        "\n",
        "# Now use encoded column for model\n",
        "X_model = X[['Age Range Encoded', 'Weight (Kg)', 'BMI', 'Cycle(R/I)',\n",
        "             'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)',\n",
        "             'Pimples(Y/N)', 'Reg.Exercise(Y/N)']]\n",
        "\n",
        "y = X['PCOS (Y/N)']\n",
        "\n",
        "# Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_model, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vD5waxC8nysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) **Model Building and Evaluation Using XGBoost Classifier**: This section involves training a predictive model using the XGBoost Classifier, known for its speed and high performance in classification tasks. After initializing the model with defined hyperparameters, it was trained on the PCOS dataset. The model’s accuracy, confusion matrix, and classification report were used to evaluate its performance in predicting PCOS, helping identify how well it distinguishes between affected and non-affected individuals.\n",
        "\n"
      ],
      "metadata": {
        "id": "dBZx5ob6QnGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "id": "wVtM0Xx9tbHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "vNV3GTHTtf6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "id": "C7eoQfE6tyu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred_xgb)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "id": "A3t5eor9t2TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) **Model Building and Evaluation Using CatBoost Classifier**:In this section, the CatBoost Classifier was used to build a predictive model for PCOS classification. CatBoost is a gradient boosting algorithm designed to handle categorical features efficiently, making it suitable for datasets with such variables. The categorical features were encoded, and the model was trained on the PCOS dataset. Following training, the model’s performance was evaluated using accuracy, confusion matrix, and classification report, providing insights into its effectiveness in distinguishing between individuals with and without PCOS."
      ],
      "metadata": {
        "id": "B8COGNdRQv76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "0C2Kn2V9vxlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade catboost\n"
      ],
      "metadata": {
        "id": "tDBDJq4_CWCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries for CatBoost\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# Convert categorical features to the 'category' data type\n",
        "categorical_features = ['Age Range', 'Cycle(R/I)', 'hair growth(Y/N)', 'Skin darkening (Y/N)',\n",
        "                        'Hair loss(Y/N)', 'Pimples(Y/N)', 'Reg.Exercise(Y/N)']\n",
        "\n",
        "# Convert the categorical columns into 'category' dtype for CatBoost\n",
        "for feature in categorical_features:\n",
        "    filtered_data[feature] = filtered_data[feature].astype('category')\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = filtered_data.drop('PCOS (Y/N)', axis=1)  # Features (all columns except 'PCOS (Y/N)')\n",
        "y = filtered_data['PCOS (Y/N)']  # Target variable (PCOS status)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost model (similar to RandomForestClassifier)\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,  # Number of boosting rounds\n",
        "    depth=6,  # Depth of trees\n",
        "    learning_rate=0.1,  # Learning rate\n",
        "    loss_function='Logloss',  # Objective function\n",
        "    cat_features=[X.columns.get_loc(c) for c in categorical_features],  # Indices of categorical features\n",
        "    random_state=42,\n",
        "    verbose=200  # Print progress every 200 iterations\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BibLKezICevo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) **Loading and Previewing Test Data**: In this section, the test data, which is structured similarly to the training dataset, is loaded from an Excel file ('Survey_data.xlsx'). The data is sourced from surveys collected online from participants, ensuring consistency in the format for model evaluation. By previewing the first few rows with head(), we can confirm the successful loading and examine the structure of the dataset to ensure it aligns with the format used in the model's training phase."
      ],
      "metadata": {
        "id": "XMZXN4zXR3Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Loading New Data\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "I8PQlojTUO4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace the filename if different\n",
        "new_data = pd.read_excel('Survey_data.xlsx', sheet_name='Clean_survey_data')\n",
        "\n",
        "# Quick preview\n",
        "new_data.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ey46ynC2V0dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) **Outlier Detection and Flagging in Test Data**:In this section, I identified and flagged outliers in the survey data to ensure data integrity for further analysis. Using Z-scores, I detected values that were more than 3 standard deviations away from the mean, marking them as outliers. I added flags for each numeric column to identify individual outliers and a column to highlight rows containing any outliers. This process helps maintain the reliability of the data before model building."
      ],
      "metadata": {
        "id": "w0A0Bn2mSrZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Fix column names (remove extra spaces)\n",
        "new_data.columns = new_data.columns.str.strip()\n",
        "\n",
        "# Select only relevant columns\n",
        "selected_columns = [\n",
        "    'PCOS (Y/N)', 'Age Range (yrs)', 'Weight (Kg)', 'Height(Cm)', 'BMI',\n",
        "    'Cycle(R/I)', 'Hip(inch)', 'Waist(inch)', 'Waist:Hip Ratio',\n",
        "    'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)',\n",
        "    'Pimples(Y/N)', 'Reg.Exercise(Y/N)'\n",
        "]\n",
        "\n",
        "selected_data = new_data[selected_columns]\n",
        "\n",
        "# Identify numeric columns for outlier detection\n",
        "numeric_cols = selected_data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Calculate Z-scores and mark outliers\n",
        "z_scores = selected_data[numeric_cols].apply(lambda x: zscore(x, nan_policy='omit'))\n",
        "\n",
        "# Define outlier threshold\n",
        "threshold = 3\n",
        "\n",
        "# Add columns to flag outliers for each numeric column\n",
        "for col in numeric_cols:\n",
        "    selected_data[f'{col}_Outlier'] = (np.abs(z_scores[col]) > threshold)\n",
        "\n",
        "# Add column to flag rows that have **any** outliers\n",
        "selected_data['Any_Outlier'] = selected_data[[f'{col}_Outlier' for col in numeric_cols]].any(axis=1)\n",
        "\n",
        "# View the dataset with outlier flags\n",
        "print(selected_data.head())\n",
        "print(\"\\nOutlier Summary:\")\n",
        "print(selected_data['Any_Outlier'].value_counts())\n",
        "\n",
        "# Check how many outliers per column\n",
        "outlier_counts = selected_data[[f'{col}_Outlier' for col in numeric_cols]].sum()\n",
        "print(\"\\nOutliers per numeric column:\\n\", outlier_counts)\n"
      ],
      "metadata": {
        "id": "cclx4n8vdMXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17) **Identifying Missing Values in the Data**: In this step, I assessed the presence of missing values in the dataset by calculating the sum of missing entries for each column."
      ],
      "metadata": {
        "id": "4UsVvXOdTNfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify Missing Values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(selected_data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "1nBZZLKC8kvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18) **Imputation of Missing Values in Numeric Columns**: Missing values in the 'Hip (inch)', 'Waist (inch)', and 'Waist:Hip Ratio' columns were imputed using the median of each column. The median was chosen to avoid the influence of outliers, ensuring a more robust and accurate dataset for further analysis."
      ],
      "metadata": {
        "id": "M5lR6PkDTtyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the 3 numeric columns with their median\n",
        "selected_data['Hip(inch)'].fillna(selected_data['Hip(inch)'].median(), inplace=True)\n",
        "selected_data['Waist(inch)'].fillna(selected_data['Waist(inch)'].median(), inplace=True)\n",
        "selected_data['Waist:Hip Ratio'].fillna(selected_data['Waist:Hip Ratio'].median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "cJoqqPnU9iC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19) **Verifying Missing Values After Imputation**: I checked the dataset again to confirm that all missing values were filled correctly, ensuring no null values remain for analysis and modeling."
      ],
      "metadata": {
        "id": "oSE0ye95UKCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying Missing Values\n",
        "print(\"\\n✅ Missing values after filling:\")\n",
        "print(selected_data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "7-z9pPjr9-p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Information Overview\n",
        "selected_data.info()"
      ],
      "metadata": {
        "id": "sLpafHideYLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20) **Defining Risk Levels Based on Probability of PCOS**: The risk_level() function categorizes the probability of PCOS into four risk levels: 'No Risk', 'Mild Risk', 'Moderate Risk', and 'Severe Risk'. Based on probability thresholds, it helps convert continuous values into actionable risk categories, aiding in easier interpretation of model predictions."
      ],
      "metadata": {
        "id": "MXX_d4qbUu0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def risk_level(prob):\n",
        "    \"\"\" Map probability of PCOS to risk categories \"\"\"\n",
        "    if prob < 0.25:\n",
        "        return 'No Risk'\n",
        "    elif prob < 0.5:\n",
        "        return 'Mild Risk'\n",
        "    elif prob < 0.75:\n",
        "        return 'Moderate Risk'\n",
        "    else:\n",
        "        return 'Severe Risk'\n"
      ],
      "metadata": {
        "id": "J7d7xdIuv8M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21) **Data Preprocessing - Renaming and Label Encoding** : The column 'Age Range (yrs)' is renamed to 'Age Range' for clarity and consistency. Label encoding is then applied to this column to convert the categorical age ranges into numerical labels, which are required for machine learning algorithms. The relevant and same features as testing data are selected for further analysis to build the predictive model."
      ],
      "metadata": {
        "id": "7_B2Q2YTVTJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the column to a cleaner name\n",
        "selected_data.rename(columns={'Age Range (yrs)': 'Age Range'}, inplace=True)\n",
        "\n",
        "\n",
        "# Make a copy of the original Age Range column from testing data\n",
        "selected_data['Age Range Original'] = selected_data['Age Range']\n",
        "\n",
        "# Apply only transform (NOT fit_transform) using encoder fitted on training data\n",
        "selected_data['Age Range Encoded'] = le.transform(selected_data['Age Range Original'].astype(str))\n",
        "\n",
        "# Show how it got encoded\n",
        "print(selected_data[['Age Range Original', 'Age Range Encoded']].head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now select your features\n",
        "features = ['Age Range Encoded', 'Weight (Kg)', 'BMI', 'Cycle(R/I)',\n",
        "            'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)',\n",
        "            'Pimples(Y/N)', 'Reg.Exercise(Y/N)']\n",
        "\n",
        "X_new = selected_data[features]\n"
      ],
      "metadata": {
        "id": "nsUL3PXXw7-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22) **Probability Prediction and Risk Classification Using Random Forest**: I used the trained Random Forest model to predict the probability of PCOS using the predict_proba method, focusing on the probability of class 1 (PCOS). This probability was added as a new column, RF_Prob. To enhance interpretability, I applied a custom risk_level function, categorizing the probability into risk levels (No Risk, Mild Risk, Moderate Risk, and Severe Risk), which was stored in the RF_Risk column. This approach provides actionable risk classifications for each individual based on their likelihood of having PCOS."
      ],
      "metadata": {
        "id": "mUyBVsl7V1cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "rf_probs = rf.predict_proba(X_new)[:, 1]  # Probability of class 1 (PCOS)\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "selected_data['RF_Prob'] = rf_probs\n",
        "selected_data['RF_Risk'] = selected_data['RF_Prob'].apply(risk_level)\n"
      ],
      "metadata": {
        "id": "SIoLTPRxw_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23) **Predicting PCOS Probability and Risk Classification with XGBoost**: I used the XGBoost model to predict the probability of PCOS (class 1) with predict_proba. The probability was added as XGB_Prob. I then applied the risk_level function to categorize the probability into risk levels (No Risk, Mild Risk, Moderate Risk, Severe Risk), storing the results in the XGB_Risk column for easier interpretation."
      ],
      "metadata": {
        "id": "57IgAQ3gAIX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "xgb_probs = xgb_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "# Add to DataFrame\n",
        "selected_data['XGB_Prob'] = xgb_probs\n",
        "selected_data['XGB_Risk'] = selected_data['XGB_Prob'].apply(risk_level)\n"
      ],
      "metadata": {
        "id": "8sscFo8txOMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24) **Handling Missing Columns and Predicting with CatBoost**: I ensured that the new dataset (X_new) included all the columns used during training, adding any missing ones with default values (e.g., np.nan). The columns were then reordered to match the training data, and categorical features were correctly typed.\n",
        "\n",
        "Using the CatBoost model, I predicted the probability of PCOS, storing it in CatBoost_Prob. I applied the risk_level function to classify the risk into categories (e.g., No Risk, Mild Risk), which was saved in the CatBoost_Risk column. This approach allowed for actionable risk predictions based on the model's output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uFWnfSK8X53Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from catboost import Pool\n",
        "\n",
        "# Get expected columns from training\n",
        "expected_columns = X_train.columns\n",
        "\n",
        "# Make a copy of X_new to avoid modifying the original\n",
        "X_new = X_new.copy()\n",
        "\n",
        "# Fix missing columns\n",
        "for col in expected_columns:\n",
        "    if col not in X_new.columns:\n",
        "        if col in categorical_features:\n",
        "            X_new.loc[:, col] = 'NaN'  # For CatBoost, use string for missing categorical\n",
        "        else:\n",
        "            X_new.loc[:, col] = np.nan  # For numerical features\n",
        "\n",
        "# Reorder columns to match training\n",
        "X_new = X_new[expected_columns]\n",
        "\n",
        "# Ensure categorical columns are strings (CatBoost requirement)\n",
        "for feature in categorical_features:\n",
        "    if feature in X_new.columns:\n",
        "        X_new.loc[:, feature] = X_new[feature].astype(str)\n",
        "\n",
        "# Get column indices of categorical features\n",
        "cat_feature_indices = [X_new.columns.get_loc(col) for col in categorical_features]\n",
        "\n",
        "# Create Pool and predict\n",
        "predict_pool = Pool(data=X_new, cat_features=cat_feature_indices)\n",
        "catboost_probs = model.predict_proba(predict_pool)[:, 1]\n",
        "\n",
        "# Update predictions\n",
        "selected_data = selected_data.copy()\n",
        "selected_data['CatBoost_Prob'] = catboost_probs\n",
        "selected_data['CatBoost_Risk'] = selected_data['CatBoost_Prob'].apply(risk_level)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ldb2WcWpxP4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25) **Displaying Final Risk Predictions and Saving Results**: In this step, I displayed the first few rows of the final dataset with the risk classifications from the three models: Random Forest (RF_Risk), XGBoost (XGB_Risk), and CatBoost (CatBoost_Risk). This provides an overview of the predicted risk levels for each individual.\n",
        "\n",
        "Additionally, I saved the updated dataset with the risk predictions to an Excel file (Survey_Predictions_with_Risk.xlsx) for further analysis and documentation. This step ensures that the results can be shared or reviewed in the future."
      ],
      "metadata": {
        "id": "qOV_10eQYloH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show final results\n",
        "print(selected_data[['RF_Risk', 'XGB_Risk', 'CatBoost_Risk']].head())\n",
        "\n",
        "# Optionally save to Excel\n",
        "selected_data.to_excel(\"Survey_Predictions_with_Risk.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "72EcV4-f0FXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26) **Post prediction exploratory analysis**:  \n",
        "\n",
        "\n",
        "1.   Visualize Risk Distribution-  how the model scored risk across the dataset.\n",
        "2.   Compare Predictions vs Actual\n",
        "3.   Risk Level vs Features- how certain features vary with the predicted risk levels.\n",
        "4.   Risk vs Categorical Features- To see if certain symptoms are more common in predicted high-risk groups.\n",
        "5.  Correlation Between Predicted Probability and Numerical Features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yOn3yBt9mMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Risk Distribution\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bar plot of predicted risk levels\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='CatBoost_Risk', data=selected_data, palette='Set2')\n",
        "plt.title(\"Distribution of Predicted Risk Levels\")\n",
        "plt.xlabel(\"Risk Level\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xQIq6xoc9LKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare predicted probabilities with actual outcome\n",
        "sns.boxplot(data=selected_data, x='PCOS (Y/N)', y='CatBoost_Prob', palette='coolwarm')\n",
        "plt.title(\"Predicted Probability by Actual PCOS Status\")\n",
        "plt.xlabel(\"Actual PCOS (Y/N)\")\n",
        "plt.ylabel(\"Predicted Probability (CatBoost)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W7C8czoT-heN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For all numerical features: Risk Level vs Features\n",
        "numerical_to_check = ['Weight (Kg)', 'BMI', 'Waist:Hip Ratio']\n",
        "\n",
        "for feature in numerical_to_check:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(data=selected_data, x='CatBoost_Risk', y=feature, palette='Set3')\n",
        "    plt.title(f'{feature} by Predicted Risk Level')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hbAfIYIa-uvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For all categorical features: Risk Level vs Features\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of categorical features to analyze\n",
        "categorical_features = [\n",
        "    'Cycle(R/I)',\n",
        "    'hair growth(Y/N)',\n",
        "    'Skin darkening (Y/N)',\n",
        "    'Hair loss(Y/N)',\n",
        "    'Pimples(Y/N)',\n",
        "    'Reg.Exercise(Y/N)',\n",
        "    'Age Range',  # if it's still categorical\n",
        "    # Add more if applicable\n",
        "]\n",
        "\n",
        "# Plot each categorical feature against predicted risk level\n",
        "plt.figure(figsize=(16, len(categorical_features) * 4))\n",
        "\n",
        "for i, col in enumerate(categorical_features, 1):\n",
        "    plt.subplot(len(categorical_features), 1, i)\n",
        "    sns.countplot(data=selected_data, x=col, hue='CatBoost_Risk', palette='Set2')\n",
        "    plt.title(f'{col} vs Predicted Risk Level')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0vXxsUjR-_B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation Between Predicted Probability and Numerical and categorical Features\n",
        "#Correlation matrix including CatBoost_Prob\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make a copy to avoid touching original data\n",
        "combined_data = selected_data.copy()\n",
        "\n",
        "# List of categorical features\n",
        "categorical_features = [\n",
        "    'Cycle(R/I)',\n",
        "    'hair growth(Y/N)',\n",
        "    'Skin darkening (Y/N)',\n",
        "    'Hair loss(Y/N)',\n",
        "    'Pimples(Y/N)',\n",
        "    'Reg.Exercise(Y/N)',\n",
        "    'Age Range'\n",
        "]\n",
        "\n",
        "# Label encode the categorical features\n",
        "le = LabelEncoder()\n",
        "for col in categorical_features:\n",
        "    combined_data[col] = le.fit_transform(combined_data[col].astype(str))\n",
        "\n",
        "# List of numerical features to include\n",
        "numerical_features = ['BMI', 'Weight (Kg)', 'Waist:Hip Ratio', 'CatBoost_Prob']\n",
        "\n",
        "# Combine all for correlation\n",
        "corr_features = categorical_features + numerical_features\n",
        "\n",
        "# Step 6: Calculate correlation matrix\n",
        "corr_matrix = combined_data[corr_features].corr()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Combined Correlation Matrix: Categorical + Numerical Features with CatBoost_Prob\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0L2SQAnA_Xun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27) **Comparative Analysis of Model Recall for PCOS Prediction** : I aimed to compare the performance of three different machine learning models—Random Forest, XGBoost, and CatBoost, for predicting PCOS.Since this is a health-related issue, my main focus was on recall, which tells us how well a model can detect actual cases of PCOS without missing them.  I prioritized recall because I believe that in real-world screening scenarios, missing a diagnosis could have serious consequences, and it's better to have false positives than false negatives in such contexts. Steps I Followed in the Code-\n",
        "\n",
        "1. **Feature Selection**\n",
        "I chose 9 common features based on domain knowledge and their relevance to PCOS (like BMI, irregular cycles, skin darkening, etc.). These features were used consistently across all three models to make the comparison fair.\n",
        "\n",
        "2. **Data Splitting**\n",
        "I used train_test_split() with stratify=y to make sure the class distribution remained balanced between training and test sets. This helps ensure that the models are evaluated on a fair and representative subset.\n",
        "\n",
        "3. **Model Training**\n",
        "\n",
        "  Random Forest:\n",
        "   I trained this model directly without extra preprocessing because it can handle numeric data and some categorical encodings easily.\n",
        "\n",
        "  XGBoost:\n",
        "   Since XGBoost doesn’t automatically handle categorical variables, I converted them into numeric codes using .astype('category').cat.codes. This allowed XGBoost to process those features properly.\n",
        "\n",
        "  CatBoost:\n",
        "   For CatBoost, which natively supports categorical features, I passed the index positions of all categorical columns so it could treat them correctly during training.\n",
        "\n",
        "4. **Evaluation**\n",
        "I used classification_report() from scikit-learn to calculate precision, recall, and F1-score for each model. I focused on the scores for class '1', which indicates the presence of PCOS. I then stored the results in a dataframe for easy comparison.\n",
        "\n",
        "5. **Visualization**\n",
        "To visually interpret the differences, I plotted the metrics using a bar chart. This helped me see how each model performed in terms of precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "JGgOM40xUBNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Split once (as before)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_common, y_common, test_size=0.2, random_state=42, stratify=y_common\n",
        ")\n",
        "\n",
        "# Train Random Forest (unchanged)\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "\n",
        "# FIX for XGBoost\n",
        "cat_cols = [\n",
        "    'Age Range','Cycle(R/I)','hair growth(Y/N)',\n",
        "    'Skin darkening (Y/N)','Hair loss(Y/N)',\n",
        "    'Pimples(Y/N)','Reg.Exercise(Y/N)'\n",
        "]\n",
        "\n",
        "# Convert categorical columns to integer codes for XGB\n",
        "for col in cat_cols:\n",
        "    X_train[col] = X_train[col].astype('category').cat.codes\n",
        "    X_test[col]  = X_test[col].astype('category').cat.codes\n",
        "\n",
        "#  Train XGBoost (drop use_label_encoder)\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "\n",
        "#  Train CatBoost (now defining cat_feats)\n",
        "#  Get the indices of the categorical features in the common_features list:\n",
        "common_features = [\n",
        "    'Age Range', 'Weight (Kg)', 'BMI', 'Cycle(R/I)',\n",
        "    'hair growth(Y/N)', 'Skin darkening (Y/N)', 'Hair loss(Y/N)',\n",
        "    'Pimples(Y/N)', 'Reg.Exercise(Y/N)'\n",
        "]\n",
        "cat_feats = [common_features.index(c) for c in cat_cols]\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    loss_function='Logloss',\n",
        "    cat_features=cat_feats,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "cb.fit(X_train, y_train)\n",
        "cb_preds = cb.predict(X_test)\n",
        "\n",
        "#  Generate and compare reports\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": rf_preds,\n",
        "    \"XGBoost\":       xgb_preds,\n",
        "    \"CatBoost\":      cb_preds\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, preds in models.items():\n",
        "    rep = classification_report(y_test, preds, output_dict=True)['1']\n",
        "    results[name] = {\n",
        "        'precision': rep['precision'],\n",
        "        'recall':    rep['recall'],\n",
        "        'f1-score':  rep['f1-score']\n",
        "    }\n",
        "\n",
        "metrics_df = pd.DataFrame(results).T\n",
        "print(metrics_df[['precision','recall','f1-score']])\n",
        "\n",
        "#  Visualize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics_df.plot(kind='bar', figsize=(8,5), colormap='Set2')\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Model Comparison: Precision / Recall / F1‑Score\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Euwy5L1G_fd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of the results**: Random Forest demonstrated the highest recall (0.853), meaning it identified the most true PCOS cases, making it a strong choice for clinical screening.\n",
        "\n",
        "XGBoost had the best F1-score (0.824), suggesting it struck the best overall balance between precision and recall.\n",
        "\n",
        "CatBoost performed slightly lower on recall but still achieved competitive precision.\n",
        "\n"
      ],
      "metadata": {
        "id": "htdYeqC-anD9"
      }
    }
  ]
}